import streamlit as st
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_core.prompts import PromptTemplate

# Configurar la p√°gina de la app
st.set_page_config(page_title="Chatbot con Google GenAI", page_icon="ü§ñ")
st.title("Chatbot con Google GenAI Langchain ü§ñ")
st.markdown("Este es un chatbot simple utilizando Google GenAI a trav√©s de Langchain y Streamlit.")

with st.sidebar:
    st.header("Configuraci√≥n del Chatbot")
    temperature = st.slider("Temperatura del modelo", 0.0, 1.0, 0.5, 0.1)
    model_name = st.selectbox("Selecciona el modelo", ["gemini-2.5-flash", "gemini-2.5-pro", "gemini-2.5-flash-lite"])

    # Inicializar el modelo de chat con la configuraci√≥n seleccionada
    chat_model = ChatGoogleGenerativeAI(model=model_name, temperature=temperature)

# Inicializar el historial de chat en la sesi√≥n
if "mensajes" not in st.session_state:
    st.session_state.mensajes = []


# Crear el template del prompt
prompt_template = PromptTemplate(
    input_variables=["mensaje", "historial"],
    template="""Eres un asistente √∫til y amigable llamado ChatBot ATM Pro. 
                
                Historial de conversaci√≥n:
                {historial}

                Responde de manera clara y concisa a la siguiente pregunta: {mensaje}"""
)

# Crear cadena usando LCEL (Langchain Expression Language)
cadena = prompt_template | chat_model

# Mostrar mensajes previos en la interfaz streamlit
for mensaje in st.session_state.mensajes:
    if isinstance(mensaje, SystemMessage):
        # No mostrar el mensaje por pantalla
        continue

    role = "assistant" if isinstance(mensaje, AIMessage) else "user"

    with st.chat_message(role):
        st.markdown(mensaje.content)

# Eliminar conversaci√≥n
if st.button("üóëÔ∏è Nueva conversaci√≥n"):
    st.session_state.mensajes = []
    st.rerun()

# Entrada de usuario
pregunta = st.chat_input("Escribe tu mensaje aqu√≠...")

if pregunta:
    # Mostrar el mensaje del usuario en la interfaz
    with st.chat_message("user"):
        st.markdown(pregunta)

    # Generar y mostrar respuesta del asistente
    try:
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            full_response = ""

            # Streaming de la respuesta
            for chunk in cadena.stream({"mensaje": pregunta, "historial": st.session_state.mensajes}):
                full_response += chunk.content
                response_placeholder.markdown(full_response + "‚ñå")  # Indicador de escritura

            response_placeholder.markdown(full_response)  # Respuesta completa sin indicador

        # A√±adir el mensaje del usuario al historial
        st.session_state.mensajes.append(HumanMessage(content=pregunta))
        # A√±adir la respuesta del asistente al historial
        st.session_state.mensajes.append(AIMessage(content=full_response))

    except Exception as e:
        st.error(f"Se produjo un error al generar la respuesta: {str(e)}")
        st.info("Por favor, verifica tu configuraci√≥n de Google GenAI y tu conexi√≥n a internet.")